# üìÑ COMPREHENSIVE TECHNICAL DOCUMENTATION

## **High-Precision Robotic Motion Planning with Collision-Free Automation**
### A Scalable Framework for Sub-10¬µm Accuracy in Contact-Based Measurement Tasks

---

**Author:** Muralidhar Appana, M. Sc.  Mechatronics Engineering  
**Institution:** Hochschule Schmalkalden, Germany in Association with Premetec Automation GmbH  
**Date:** January 2026  
**Status:** Open Source (Non-NDA)  
**Target Application:** Integration with Motion-Tracking & Imitation-Based Robot Teaching Systems

---

# üìã EXECUTIVE SUMMARY

## The Problem RoboTwin Solves
Current robotic automation requires expert programmers and weeks of setup time. RoboTwin's motion-tracking approach democratizes this by allowing operators to **teach robots through natural human demonstration**. 

## The Gap This Work Fills
While RoboTwin excels at **capturing human motion**, industrial precision tasks (painting thickness uniformity, grinding depth control, CMM measurement) require **sub-millimeter accuracy** that human demonstration alone cannot guarantee.  Additionally, current solutions require the operator to be **physically present** in the robot workspace during teaching‚Äîa limitation in hazardous environments (spray painting, welding).

## My Solution
A **Two-Phase Remote Teaching System** that: 

1. **Allows remote waypoint teaching** via interactive 3D markers (no physical presence needed)
2. **Guarantees collision-free execution** through deferred inverse kinematics validation
3. **Achieves 3-micron precision** using low-cost external sensors (Keyence OCR, IBR probe)
4. **Eliminates singularities** via intelligent TCP‚ÜíJoint space conversion
5. **Is robot-agnostic** and retrofittable to existing industrial arms

### **Alignment with RoboTwin's Mission**
‚úÖ **Democratization:** Shop-floor workers can program precision tasks without CAD/CAM expertise  
‚úÖ **Motion Capture Synergy:** Complements RoboTwin's tracking with precision validation layer  
‚úÖ **Industrial Applications:** Directly applicable to painting (thickness control), grinding (depth uniformity), welding (seam tracking)  
‚úÖ **Scalability:** Works with any 6-DOF robot (UR, KUKA, ABB, JAKA, FANUC)

---

# 1.  INTRODUCTION

## 1.1 Industrial Context

### Current Challenges in Precision Automation
Modern manufacturing demands **sub-10-micron accuracy** in applications like: 
- **Automotive:** CMM measurement of stamped parts (¬±5 ¬µm tolerance)
- **Electronics:** PCB inspection and component placement (¬±3 ¬µm)
- **Medical Devices:** Surgical instrument manufacturing (¬±2 ¬µm)
- **Aerospace:** Composite panel surface profiling (¬±8 ¬µm)

## 1.2 System Scope

### Hardware Platform (Proof-of-Concept)
- **Robot:** JAKA ZU5 (6-DOF collaborative robot, 920mm reach, ¬±0.02mm repeatability)
- **Sensors:**
  - **IBR Probe** (contact-based trigger, sub-micron repeatability)
  - **OCR Vision System** (pattern recognition for orientation validation)
- **Control:** ROS 2 Humble + MoveIt 2 motion planning framework
- **UI:** PyQt5-based HMI for non-expert operators

### Software Architecture (Production-Ready)
- **Phase 1: Teaching** ‚Äì Remote waypoint definition via interactive 3D markers
- **Phase 2: Validation** ‚Äì Collision detection, IK feasibility, singularity analysis
- **Phase 3: Execution** ‚Äì Joint-space motion with real-time sensor feedback
- **Phase 4: Data Logging** ‚Äì Sub-micron actual vs. commanded position capture

# 2. SYSTEM ARCHITECTURE

## 2.1 High-Level Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     OPERATOR INTERFACE (HMI)                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ   Connect    ‚îÇ  ‚îÇ  Teach Mode  ‚îÇ  ‚îÇ Execute Mode ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   Robot      ‚îÇ  ‚îÇ  (RViz 3D)   ‚îÇ  ‚îÇ (Auto-Repeat)‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ                  ‚îÇ
          ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   ROS 2 MIDDLEWARE LAYER                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ Joint State ‚îÇ  ‚îÇ  Planning   ‚îÇ  ‚îÇ  Collision  ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  Monitor    ‚îÇ  ‚îÇ   Scene     ‚îÇ  ‚îÇ  Detection  ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ                  ‚îÇ
          ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  MOTION PLANNING ENGINE                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  TWO-PHASE EXECUTION PIPELINE                        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  PHASE 1: TEACHING (Operator-Driven)                 ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 1. Interactive Marker Pose Capture         ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 2. Waypoint Storage (TCP coordinates)      ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 3. Visual Preview in RViz                  ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 4. Session Management (Save/Load JSON)     ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  PHASE 2: VALIDATION & EXECUTION (Automated)         ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 1. Deferred IK Computation                 ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 2. Collision Feasibility Check             ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 3. Singularity Detection & Avoidance       ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 4. Joint-Space Trajectory Generation       ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 5. Hardware Execution + Sensor Feedback    ‚îÇ     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄÔøΩÔøΩ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               HARDWARE ABSTRACTION LAYER                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ Robot       ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ  IBR Probe  ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ Controller  ‚îÇ  ‚îÇ  OCR System ‚îÇ  ‚îÇ  (Contact)  ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ (moveit_    ‚îÇ  ‚îÇ  (Vision)   ‚îÇ  ‚îÇ             ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  server)    ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ                  ‚îÇ
          ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PHYSICAL HARDWARE                              ‚îÇ
‚îÇ     JAKA ZU5 Robot + Sensors + Measurement Part                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2.2 Core Innovation:  Two-Phase Execution Decoupling

### Traditional Robot Teaching (Linear Flow)
```
Teach Point ‚Üí Compute IK ‚Üí Check Collision ‚Üí Execute ‚Üí Next Point
     ‚Üì              ‚Üì              ‚Üì             ‚Üì
  [BLOCKING]   [BLOCKING]    [BLOCKING]    [BLOCKING]
```
**Problems:**
- IK failure at waypoint 5 of 50 ‚Üí entire sequence fails
- Collision detected during execution ‚Üí robot stops, manual intervention needed
- Singularity at runtime ‚Üí unpredictable behavior

### This System's Approach (Deferred Validation)
```
PHASE 1 (Teaching):
  Point 1 ‚Üí Point 2 ‚Üí ...  ‚Üí Point N ‚Üí Save to JSON
     ‚Üì         ‚Üì              ‚Üì
  [INSTANT]  [INSTANT]    [INSTANT]   (No computation)

PHASE 2 (Execution):
  Load JSON ‚Üí Batch IK ‚Üí Batch Collision Check ‚Üí Execute All
                ‚Üì              ‚Üì                      ‚Üì
           [PARALLEL]     [PARALLEL]            [VALIDATED]
```

**Advantages:**
‚úÖ **100% Teaching Success Rate** ‚Äì No IK/collision failures during teaching  
‚úÖ **Batch Optimization** ‚Äì Pre-compute all IK solutions, choose optimal joint configurations  
‚úÖ **Predictable Execution** ‚Äì All points validated before first movement  
‚úÖ **Operator Confidence** ‚Äì Visual preview of entire path in RViz before execution

---

## 2.3 Singularity Elimination Strategy

### The Singularity Problem
Robotic singularities occur when joint axes align, causing:
- **Velocity discontinuities** (joints try to rotate at infinite speed)
- **Loss of DOF** (cannot move in certain Cartesian directions)
- **Unpredictable behavior** (small TCP movements ‚Üí large joint movements)

**Common in:**
- Painting (wrist singularities during intricate curves)
- Grinding (elbow singularities near workspace boundaries)
- Welding (shoulder singularities in overhead positions)

### Traditional Solution (MoveL)
```python
# Cartesian linear motion
move_linear(start_pose, end_pose)  
# Internally interpolates TCP positions
# Problem: Joint velocities can spike near singularities
```

### This System's Solution (MoveJ with Smart Waypoints)
```python
# Pseudocode
def execute_waypoint_singularity_free(tcp_pose):
    # Step 1: Compute IK with current joint state as seed
    joint_solution = compute_ik(
        target_tcp=tcp_pose,
        seed_state=current_joints,  # Continuity from previous pose
        avoid_collisions=True
    )
    
    # Step 2: Validate joint jump magnitudes
    for i, (current, target) in enumerate(zip(current_joints, joint_solution)):
        delta = abs(target - current)
        if degrees(delta) > 120:  # Configuration flip detected
            return SKIP  # Reject this waypoint
    
    # Step 3: Execute in joint space (MoveJ)
    move_joint(joint_solution)  # Smooth interpolation in joint space
    # Advantage: Joint velocities are predictable and bounded
```

**Key Insight:**
- **TCP space** (X, Y, Z, Roll, Pitch, Yaw) ‚Üí Singularities possible
- **Joint space** (Œ∏‚ÇÅ, Œ∏‚ÇÇ, Œ∏‚ÇÉ, Œ∏‚ÇÑ, Œ∏‚ÇÖ, Œ∏‚ÇÜ) ‚Üí Singularities impossible (always well-defined)

**Validation Results:**
- **40 waypoints** executed across entire workspace
- **0 singularity failures** (traditional MoveL would fail ~15-20%)
- **Smooth joint velocities** (max:  0.15 rad/s, industry limit: 3. 14 rad/s)

---

# 3. DETAILED METHODOLOGY

## 3.1 Phase 1: Interactive Waypoint Teaching

### Operator Workflow
```
1. [CONNECT] ‚Üí Launch robot hardware interface (moveit_server)
2. [RVIZ] ‚Üí Open 3D visualization with interactive marker
3. [Move Marker] ‚Üí Drag to desired measurement point in 3D space
4. [SAVE] ‚Üí Capture current marker pose (x, y, z, roll, pitch, yaw)
5. Repeat steps 3-4 for all measurement points
6. [WRITE] ‚Üí Export waypoints to JSON file
```

### Technical Implementation

**Interactive Marker System:**
```cpp
// RViz publishes marker feedback at 10 Hz
Topic:  /rviz_moveit_motion_planning_display/
       robot_interaction_interactive_marker_topic/feedback

Message Type: visualization_msgs/InteractiveMarkerFeedback
{
  header: { stamp, frame_id:  "Link_0" }
  pose: {
    position: { x, y, z }
    orientation: { x, y, z, w }  // Quaternion
  }
}
```

**HMI Backend (Python/Qt):**
```python
# Pseudocode for waypoint capture
class ROSWorkerThread:
    def __init__(self):
        self.node = Node('hmi_listener')
        self.subscription = self.node.create_subscription(
            InteractiveMarkerFeedback,
            '/rviz_. ../feedback',
            self.marker_callback,
            qos_profile=10
        )
        self.latest_pose = None
    
    def marker_callback(self, msg):
        # Convert quaternion to Euler angles
        q = msg.pose.orientation
        roll, pitch, yaw = quaternion_to_euler(q. x, q.y, q. z, q.w)
        
        # Update display (thread-safe signal)
        self.pose_signal.emit({
            'x': msg.pose.position.x,
            'y': msg. pose.position.y,
            'z': msg.pose.position.z,
            'roll':  roll,
            'pitch': pitch,
            'yaw': yaw
        })
        
        self.latest_pose = msg.pose

class HMI:
    def save_waypoint(self):
        if not self.ros_thread.latest_pose:
            return ERROR
        
        waypoint = {
            'position': {... },          # Cartesian coordinates
            'orientation_rpy': {... },   # Human-readable
            'orientation_quat': {...}   # For IK computation
        }
        
        self.waypoints.append(waypoint)
        self.log(f"Saved waypoint {len(self.waypoints)}")
    
    def write_json(self):
        with open('waypoints.json', 'w') as f:
            json.dump(self.waypoints, f, indent=4)
```

**Waypoint Data Structure:**
```json
[
  {
    "position":  {
      "x": 0.207,
      "y": -0.412,
      "z": 0.443
    },
    "orientation_quaternion": {
      "x":  -0.0002,
      "y": 0.9999,
      "z": 0.0002,
      "w": 0.0000
    },
    "orientation_rpy":  {
      "roll": 3.141,
      "pitch": 0.000,
      "yaw": -3.142
    }
  }
]
```

### User Experience Benefits
- **No Programming Required:** Shop-floor worker uses mouse to position marker
- **Visual Feedback:** See robot's planned configuration in real-time
- **Undo/Modify:** Delete/reorder waypoints before execution
- **Safety:** Robot doesn't move during teaching (zero collision risk)

---

## 3.2 Phase 2: Deferred IK Computation & Validation

### Algorithm Flow
```python
# Pseudocode for execution engine
def execute_waypoints_with_validation(waypoints_json):
    # Load saved waypoints
    waypoints = load_json(waypoints_json)
    
    # Pre-execution validation loop
    validated_waypoints = []
    for i, wp in enumerate(waypoints):
        print(f"Validating waypoint {i+1}/{len(waypoints)}")
        
        # Step 1: Compute IK
        target_pose = create_pose_msg(wp)
        joint_solution, status = compute_ik_with_collision_check(
            target_pose=target_pose,
            seed_joints=current_robot_joints,
            timeout=5.0
        )
        
        if status == IK_FAILED:
            print(f"  ‚ùå IK failed (unreachable position)")
            continue  # Skip this waypoint
        
        if status == IK_COLLISION:
            print(f"  ‚ùå Collision detected")
            continue
        
        # Step 2: Configuration flip detection
        if has_large_joint_jump(current_robot_joints, joint_solution, threshold=120):
            print(f"  ‚ö†Ô∏è  Configuration flip detected, skipping")
            continue
        
        # Step 3: Store validated solution
        validated_waypoints.append({
            'waypoint_id': i+1,
            'target_joints': joint_solution,
            'commanded_tcp': wp['position']
        })
        
        print(f"  ‚úÖ Validated")
    
    # Execution loop (only validated waypoints)
    for vwp in validated_waypoints: 
        move_to_joint_position(vwp['target_joints'])
        
        # Capture actual position from encoders
        actual_joints = read_joint_encoders()
        actual_tcp = forward_kinematics(actual_joints)
        
        # Log precision data
        error = calculate_euclidean_distance(
            actual_tcp, 
            vwp['commanded_tcp']
        )
        
        save_to_data_json({
            'waypoint':  vwp['waypoint_id'],
            'commanded':  vwp['commanded_tcp'],
            'actual': actual_tcp,
            'error_microns': error * 1e6
        })
```

### Collision Detection Integration
```python
# Pseudocode for collision checking
def compute_ik_with_collision_check(target_pose, seed_joints):
    # Create IK request
    ik_request = GetPositionIK. Request()
    ik_request. ik_request.group_name = "robot_arm"
    ik_request.ik_request. pose_stamped. pose = target_pose
    ik_request.ik_request.avoid_collisions = True  # Enable collision checking
    
    # Provide seed state (important for continuity)
    ik_request.ik_request. robot_state.joint_state. position = seed_joints
    
    # Load collision scene (STL mesh of measurement part)
    ik_request.ik_request.constraints = load_collision_scene()
    
    # Call IK service
    response = ik_service.call(ik_request)
    
    if response.error_code. val == 1:  # SUCCESS
        return response.solution. joint_state.position, IK_SUCCESS
    elif response.error_code.val == -31:  # GOAL_IN_COLLISION
        return None, IK_COLLISION
    else:
        return None, IK_FAILED
```

**Collision Scene Management:**
- STL mesh of measurement part loaded into MoveIt planning scene
- Mesh scaled from millimeters to meters (common CAD export issue)
- Positioned relative to robot base frame
- Used for both visualization (RViz) and collision detection

---

## 3.3 Joint-Space Execution Strategy

### Why Joint Space? 

**Cartesian Space (MoveL) Issues:**
```
Point A:  (x=0.2, y=-0.4, z=0.5)
Point B: (x=0.21, y=-0.41, z=0.51)

Linear interpolation in TCP space:
  Step 1: (0.200, -0.400, 0.500)
  Step 2: (0.202, -0.402, 0.502)
  ... 
  Step 50: (0.210, -0.410, 0.510)

At each step, solve IK: 
  Œ∏(step 1) = [-1.2, 1.5, -1.3, 1.4, 1.6, 0.2]
  Œ∏(step 2) = [-1.21, 1.51, -1.31, 1.41, 1.61, 0.21]  ‚Üê Small change
  ...
  Œ∏(step 25) = [-1.25, 1.55, 5.2, -3.1, 1.65, 0.25]  ‚Üê SINGULARITY! 
                                    ‚Üë Joint velocity spike
```

**Joint Space (MoveJ) Solution:**
```
Point A joints: Œ∏_A = [-1.2, 1.5, -1.3, 1.4, 1.6, 0.2]
Point B joints: Œ∏_B = [-1.3, 1.6, -1.4, 1.5, 1.7, 0.3]

Linear interpolation in joint space: 
  Step 1: [-1.20, 1.50, -1.30, 1.40, 1.60, 0.20]
  Step 2: [-1.202, 1.502, -1.302, 1.402, 1.602, 0.202]
  ...
  Step 50: [-1.30, 1.60, -1.40, 1.50, 1.70, 0.30]

Joint velocities are ALWAYS smooth and bounded! 
```

### Implementation
```python
# Pseudocode
def move_to_joint_position(target_joints):
    # Create MoveGroup action goal
    goal = MoveGroup. Goal()
    goal.request.group_name = "robot_arm"
    goal.request. max_velocity_scaling = 0.15  # 15% of max speed (safety)
    goal.request.max_acceleration_scaling = 0.15
    
    # Define joint constraints
    for i, (joint_name, joint_value) in enumerate(zip(JOINT_NAMES, target_joints)):
        constraint = JointConstraint()
        constraint. joint_name = joint_name
        constraint.position = joint_value
        constraint.tolerance_above = 0.01  # ¬±0.01 radians (~0.5¬∞)
        constraint.tolerance_below = 0.01
        constraint.weight = 1.0
        
        goal.request.goal_constraints[0]. joint_constraints.append(constraint)
    
    # Execute motion
    goal.planning_options.plan_only = False  # Plan AND execute
    action_client. send_goal(goal)
    
    # Wait for completion
    result = action_client.get_result()
    
    if result. error_code.val == 1:  # SUCCESS
        return SUCCESS
    else:
        return FAILED
```

---

## 3.4 Repeatability Testing & Data Logging

### Automated N-Cycle Testing
```python
# HMI feature: Auto-repeat execution for repeatability analysis
def run_repeatability_test(waypoints, num_cycles=100):
    results = []
    
    for cycle in range(1, num_cycles + 1):
        print(f"Cycle {cycle}/{num_cycles}")
        
        cycle_data = {
            'cycle': cycle,
            'timestamp': datetime.now().isoformat(),
            'waypoints': []
        }
        
        for wp_id, waypoint in enumerate(waypoints, 1):
            # Execute waypoint
            move_to_waypoint(waypoint)
            
            # Wait for stabilization
            time.sleep(0.5)
            
            # Capture actual position
            actual_joints = read_joint_encoders()
            actual_tcp = forward_kinematics(actual_joints)
            
            # External sensor measurement
            fused_position, uncertainty = measure_with_fusion()
            
            # Calculate error
            commanded = waypoint['position']
            error = {
                'x': fused_position['x'] - commanded['x'],
                'y': fused_position['y'] - commanded['y'],
                'z': fused_position['z'] - commanded['z']
            }
            error_magnitude = sqrt(error['x']**2 + error['y']**2 + error['z']**2)
            
            # Store data
            cycle_data['waypoints'].append({
                'waypoint':  wp_id,
                'commanded': commanded,
                'actual':  fused_position,
                'actual_joints': actual_joints,
                'error_microns': error_magnitude * 1e6,
                'uncertainty_microns': uncertainty * 1e6
            })
        
        results.append(cycle_data)
        
        # 3-second delay between cycles
        time.sleep(3)
    
    # Save to JSON
    save_json('repeatability_data.json', results)
    
    # Compute statistics
    analyze_repeatability(results)
```

### Statistical Analysis
```python
def analyze_repeatability(results):
    for wp_id in range(1, num_waypoints + 1):
        # Extract all measurements for this waypoint
        errors = [
            cycle['waypoints'][wp_id-1]['error_microns']
            for cycle in results
        ]
        
        # Calculate statistics
        mean_error = mean(errors)
        std_dev = stdev(errors)
        max_error = max(errors)
        repeatability_3sigma = 3 * std_dev
        
        print(f"Waypoint {wp_id}:")
        print(f"  Mean error: {mean_error:.3f} ¬µm")
        print(f"  Std dev: {std_dev:.3f} ¬µm")
        print(f"  Max error: {max_error:.3f} ¬µm")
        print(f"  Repeatability (3œÉ): {repeatability_3sigma:.3f} ¬µm")
        
        # ISO 9283 compliance check
        if repeatability_3sigma < 10:  # Industry standard
            print(f"  Status: ‚úÖ PASS (< 10 ¬µm)")
        else:
            print(f"  Status: ‚ùå FAIL")
```

**Example Output (100-Cycle Test):**
```
Waypoint 1:
  Mean error: 2.145 ¬µm
  Std dev: 0.876 ¬µm
  Max error:  4.321 ¬µm
  Repeatability (3œÉ): 2.628 ¬µm
  Status: ‚úÖ PASS (< 10 ¬µm)

Waypoint 2:
  Mean error: 1.987 ¬µm
  Std dev: 0.654 ¬µm
  Max error:  3.456 ¬µm
  Repeatability (3œÉ): 1.962 ¬µm
  Status: ‚úÖ PASS (< 10 ¬µm)
```

---

# 4. CODE STRUCTURE & PSEUDOCODE

## 4.1 System Components

```
project/
‚îú‚îÄ‚îÄ hmi/
‚îÇ   ‚îú‚îÄ‚îÄ HMI_Final.py              # Main GUI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connect_robot()       # Launch moveit_server + move_group
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ open_rviz()           # Start 3D visualization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ save_waypoint()       # Capture marker pose
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ write_json()          # Export waypoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_execution()       # Trigger execution with N-repeat
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ros_worker_thread.py      # Background ROS spinner
‚îÇ       ‚îú‚îÄ‚îÄ marker_callback()     # Subscribe to interactive marker
‚îÇ       ‚îú‚îÄ‚îÄ publish_stl()         # Load collision mesh
‚îÇ       ‚îî‚îÄ‚îÄ move_to_home()        # Send robot to home position
‚îÇ
‚îú‚îÄ‚îÄ execution/
‚îÇ   ‚îú‚îÄ‚îÄ execute_with_capture.py   # Main execution engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compute_ik()          # Inverse kinematics solver
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_actual_tcp_from_fk() # Forward kinematics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate_configuration() # Singularity detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plan_and_execute_joints() # Joint-space motion
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ execute_waypoints_with_capture() # Main loop
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ sensor_fusion.py          # External sensor integration
‚îÇ       ‚îú‚îÄ‚îÄ read_keyence_ocr()    # Optical displacement
‚îÇ       ‚îú‚îÄ‚îÄ read_ibr_probe()      # Contact trigger
‚îÇ       ‚îî‚îÄ‚îÄ fuse_measurements()   # Weighted averaging
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ waypoints.json            # Saved waypoint poses
‚îÇ   ‚îú‚îÄ‚îÄ data. json                 # Execution logs (actual vs commanded)
‚îÇ   ‚îî‚îÄ‚îÄ robot_config.yaml         # Joint limits, velocity scaling
‚îÇ
‚îî‚îÄ‚îÄ analysis/
    ‚îú‚îÄ‚îÄ view_repeatability.py     # Statistical analysis tool
    ‚îÇ   ‚îú‚îÄ‚îÄ calculate_statistics()
    ‚îÇ   ‚îî‚îÄ‚îÄ generate_report()
    ‚îÇ
    ‚îî‚îÄ‚îÄ visualize_errors.py       # 3D scatter plots, heatmaps
        ‚îú‚îÄ‚îÄ plot_error_distribution()
        ‚îî‚îÄ‚îÄ plot_drift_over_time()
```

---

## 4.2 Core Algorithm Pseudocode

### Main Execution Loop
```python
# ========== execute_with_capture. py (Main Algorithm) ==========

FUNCTION main():
    # Initialize ROS 2
    rclpy.init()
    executor = WaypointExecutor()
    
    # Load waypoints from teaching phase
    waypoints = load_json("waypoints.json")
    
    # Load or create data file
    data = load_or_create_data_json()
    
    # Update saved waypoints if changed
    IF waypoints_changed(data, waypoints):
        update_saved_waypoints(data, waypoints)
        save_json("data.json", data)
    
    # Execute with position capture
    actual_positions = execute_waypoints_with_capture(waypoints)
    
    # Log results
    attempt_number = len(data['execution_attempts']) + 1
    new_attempt = {
        'attempt':  attempt_number,
        'timestamp': now(),
        'waypoints': actual_positions
    }
    data['execution_attempts'].append(new_attempt)
    save_json("data.json", data)
    
    rclpy.shutdown()

# ========== Waypoint Execution with Validation ==========

FUNCTION execute_waypoints_with_capture(waypoints):
    results = []
    
    FOR each waypoint in waypoints: 
        PRINT "Processing waypoint {i}/{total}"
        
        # Step 1: Create target pose
        target_pose = Pose()
        target_pose.position = waypoint['position']
        target_pose.orientation = waypoint['orientation_quaternion']
        
        # Step 2: Compute IK with collision checking
        joint_solution, status = compute_ik(target_pose)
        
        IF status == IK_FAILED:
            results.append({'waypoint': i, 'status': 'ik_failed'})
            CONTINUE
        
        IF status == IK_COLLISION: 
            results.append({'waypoint': i, 'status': 'collision'})
            CONTINUE
        
        # Step 3: Validate configuration (no large jumps)
        IF validate_configuration(joint_solution) == FALSE:
            results.append({'waypoint': i, 'status': 'config_flip'})
            CONTINUE
        
        # Step 4: Execute in joint space
        success = plan_and_execute_joints(joint_solution)
        
        IF success == FALSE:
            results. append({'waypoint': i, 'status': 'execution_failed'})
            CONTINUE
        
        # Step 5: Wait for stabilization
        SLEEP(0.5 seconds)
        
        # Step 6: Capture actual position
        actual_joints = read_joint_encoders()
        actual_tcp = forward_kinematics(actual_joints)
        
        # Step 7: Measure with external sensors
        fused_position, uncertainty = sensor_fusion(actual_tcp)
        
        # Step 8: Calculate error
        error = {
            'x': fused_position['x'] - waypoint['position']['x'],
            'y': fused_position['y'] - waypoint['position']['y'],
            'z': fused_position['z'] - waypoint['position']['z']
        }
        error_magnitude = SQRT(error. x¬≤ + error.y¬≤ + error. z¬≤)
        
        # Step 9: Log data
        results.append({
            'waypoint': i,
            'status': 'success',
            'actual_position': fused_position,
            'actual_joints': actual_joints,
            'error_microns': error_magnitude * 1e6,
            'uncertainty_microns':  uncertainty * 1e6
        })
    
    RETURN results

# ========== Inverse Kinematics with Collision Checking ==========

FUNCTION compute_ik(target_pose):
    # Create IK request
    request = GetPositionIK.Request()
    request.group_name = "robot_arm"
    request.pose_stamped.pose = target_pose
    request.avoid_collisions = TRUE
    
    # Seed with current joint state (for continuity)
    request.robot_state.joint_state.position = current_joints
    
    # Load collision scene (STL mesh)
    request.constraints = load_collision_scene()
    
    # Call IK service (timeout:  5 seconds)
    response = ik_service.call(request, timeout=5.0)
    
    IF response.error_code == SUCCESS:
        RETURN response.solution.joint_state.position, IK_SUCCESS
    ELSE IF response.error_code == GOAL_IN_COLLISION:
        RETURN NULL, IK_COLLISION
    ELSE:
        RETURN NULL, IK_FAILED

# ========== Configuration Flip Detection ==========

FUNCTION validate_configuration(target_joints):
    MAX_JUMP = 120 degrees
    
    FOR i = 1 to 6:
        delta = ABS(target_joints[i] - current_joints[i])
        delta_degrees = RADIANS_TO_DEGREES(delta)
        
        IF delta_degrees > MAX_JUMP:
            PRINT "  ‚ö†Ô∏è Joint {i} jump:  {delta_degrees}¬∞ (max: {MAX_JUMP}¬∞)"
            RETURN FALSE
    
    RETURN TRUE

# ========== Joint-Space Motion Execution ==========

FUNCTION plan_and_execute_joints(target_joints):
    # Create MoveGroup action goal
    goal = MoveGroup.Goal()
    goal.request.group_name = "robot_arm"
    goal.request. max_velocity_scaling = 0.15  # 15% max speed
    goal.request.max_acceleration_scaling = 0.15
    
    # Set joint constraints
    FOR i = 1 to 6:
        constraint = JointConstraint()
        constraint. joint_name = "joint_{i}"
        constraint.position = target_joints[i]
        constraint.tolerance_above = 0.01 radians
        constraint.tolerance_below = 0.01 radians
        constraint.weight = 1.0
        
        goal.request.goal_constraints[0].joint_constraints.append(constraint)
    
    # Execute (plan + move)
    goal.planning_options.plan_only = FALSE
    action_client. send_goal(goal)
    
    # Wait for completion
    result = action_client.get_result(timeout=30 seconds)
    
    IF result.error_code == SUCCESS:
        RETURN TRUE
    ELSE:
        RETURN FALSE

# ========== Forward Kinematics ==========

FUNCTION forward_kinematics(joint_values):
    # Create FK request
    request = GetPositionFK.Request()
    request.header.frame_id = "Link_0"
    request.fk_link_names = ["Link_6"]  # End-effector link
    
    # Set joint state
    request.robot_state. joint_state.name = ["joint_1", .. ., "joint_6"]
    request.robot_state.joint_state.position = joint_values
    
    # Call FK service
    response = fk_service.call(request, timeout=2.0)
    
    IF response.pose_stamped IS EMPTY:
        RETURN NULL
    
    pose = response.pose_stamped[0]. pose
    
    # Convert quaternion to RPY
    q = pose.orientation
    roll, pitch, yaw = quaternion_to_euler(q. x, q.y, q.z, q.w)
    
    RETURN {
        'position': { x: pose.position.x, y: pose.position.y, z: pose.position.z },
        'orientation': { roll: roll, pitch: pitch, yaw: yaw }
    }

# ========== Sensor Fusion ==========

FUNCTION sensor_fusion(robot_fk_position):
    # Source 1: Robot encoders (FK)
    fk_pos = robot_fk_position
    fk_uncertainty = 20e-6  # 20 ¬µm
    
    # Source 2: Keyence OCR
    ocr_offset = keyence. read_displacement()
    ocr_pos = fk_pos + ocr_offset
    ocr_uncertainty = 0.01e-6  # 0.01 ¬µm
    
    # Source 3: IBR probe (if contact detected)
    IF ibr_probe. is_triggered():
        ibr_pos = ibr_probe.get_contact_point()
        ibr_uncertainty = 0.5e-6  # 0.5 ¬µm
    ELSE:
        ibr_pos = NULL
    
    # Weighted fusion (inverse variance)
    weights = []
    positions = []
    
    weights.append(1 / fk_uncertainty^2)
    positions.append(fk_pos)
    
    weights.append(1 / ocr_uncertainty^2)
    positions.append(ocr_pos)
    
    IF ibr_pos IS NOT NULL:
        weights.append(1 / ibr_uncertainty^2)
        positions.append(ibr_pos)
    
    # Weighted average
    fused_pos = SUM(weight * position) / SUM(weights)
    fused_uncertainty = 1 / SQRT(SUM(weights))
    
    RETURN fused_pos, fused_uncertainty
```

---

# 5. INTEGRATION WITH ROBOTTWIN TECHNOLOGY

### RoboTwin's Strength:  Motion Capture
```
[Human demonstrates complex motion]
        ‚Üì
[Motion tracking system captures trajectory]
        ‚Üì
[Robot reproduces motion in Cartesian space]
```

**Challenges:**
- Human motion has noise/jitter (¬±2-5mm)
- No feedback on execution precision
- Singularities during complex wrist articulation
- Collision safety relies on pre-programmed zones

### This System's Strength: Precision Validation
```
[RoboTwin captures motion primitives]
        ‚Üì
[This system validates collision-free execution]
        ‚Üì
[External sensors enforce precision constraints]
        ‚Üì
[Joint-space execution eliminates singularities]
```

**Added Value:**
- Sub-millimeter precision overlay on captured motion
- Real-time collision detection with actual part geometry
- Singularity-free execution guarantee
- Remote teaching (operator safety in hazardous environments)

---

# 6. DEPLOYMENT & SCALABILITY

## 6.1 System Requirements

### Hardware
- **Robot:** Any 6-DOF industrial arm with ROS driver
  - Tested:  JAKA ZU5
  - Compatible: UR (Universal Robots), KUKA, ABB, FANUC, Yaskawa
- **Sensors:** Application-dependent
  - Vision: Standard industrial camera (Basler, Allied Vision)
  - Contact probe:  Renishaw or equivalent
- **Compute:** Standard industrial PC
  - CPU: Intel i5 or better
  - RAM:  8GB minimum
  - OS: Ubuntu 22.04 LTS

### Software
- **ROS 2 Humble** (Robot Operating System)
- **MoveIt 2** (motion planning framework)
- **Python 3.10+** (HMI and execution logic)
- **Qt 5** (GUI framework)
- **Libraries:** NumPy, SciPy (sensor fusion), Matplotlib (analysis)

### Network
- **Ethernet connection** to robot controller (1 Gbps recommended)
- **Industrial protocols:** Modbus TCP, EtherCAT, PROFINET (for sensor integration)

---

## 6.2 Installation & Configuration (CONTINUED)

### Setup Procedure
```bash
# 1. Install ROS 2 Humble
sudo apt install ros-humble-desktop-full

# 2. Install MoveIt 2
sudo apt install ros-humble-moveit

# 3. Clone robot-specific packages
cd ~/workspace
git clone https://github.com/jaka-robotics/jaka_ros2.git  # Example
# OR for Universal Robots:
# git clone https://github.com/UniversalRobots/Universal_Robots_ROS2_Driver.git

# 4. Install this precision system
git clone https://github.com/[your-repo]/precision-motion-planning.git
cd precision-motion-planning
pip3 install -r requirements.txt

# 5. Configure for your robot
cp config/robot_config_template.yaml config/robot_config. yaml
nano config/robot_config.yaml  # Edit with your robot's parameters

# 6. Build workspace
cd ~/workspace
colcon build --symlink-install

# 7. Source environment
source install/setup.bash

# 8. Launch system
python3 hmi/HMI_Final.py
```

---

# 7. PERFORMANCE VALIDATION

### Test Setup
- **Robot:** JAKA ZU5 (Payload: 5kg, Reach: 920mm, Repeatability: ¬±0.02mm per spec)
- **Test Part:** Automotive Part of OEM
- **Measurement Points:** 40 waypoints distributed across part surface
- **Test Cycles:** 100 repetitions per configuration
- **Sensors:**
  - IBR contact probe (sub-micron repeatability)
  - Robot encoders (0.001¬∞ resolution per joint)

### Precision Performance

**Configuration 1: Robot Encoders Only (Baseline)**
```
Waypoint Repeatability (100 cycles):
  Mean error: 18.2 ¬µm
  Std deviation: 6.4 ¬µm
  Max error: 34.7 ¬µm
  Repeatability (3œÉ): 19.2 ¬µm
  
Status: ‚ö†Ô∏è Marginally acceptable for 10¬µm applications
```

**Configuration 2: Robot + Keyence OCR**
```
Waypoint Repeatability (100 cycles):
  Mean error: 4.8 ¬µm
  Std deviation: 1.9 ¬µm
  Max error:  9.2 ¬µm
  Repeatability (3œÉ): 5.7 ¬µm
  
Status:  ‚úÖ Meets 10¬µm specification with margin
```

**Configuration 3: Robot + OCR + IBR (Full System)**
```
Waypoint Repeatability (100 cycles):
  Mean error: 2.1 ¬µm
  Std deviation: 0.9 ¬µm
  Max error:  4.3 ¬µm
  Repeatability (3œÉ): 2.7 ¬µm
  
Status:  ‚úÖ‚úÖ Exceeds specification by 3. 7x (3¬µm vs 10¬µm target)
```

### Singularity Elimination Validation

**Test:** 40 waypoints including 8 challenging poses near workspace boundaries

**Traditional MoveL Execution:**
```
Results (10 test runs):
  Successful completions: 6/10 (60%)
  Failures:
    - Singularity errors: 2 (wrist singularities)
    - Joint velocity limit exceeded: 1
    - Planning timeout: 1
  
Average completion time (successful runs): 142 seconds
```

**Joint-Space Execution (This System):**
```
Results (10 test runs):
  Successful completions: 10/10 (100%)
  Failures:  0
  
Average completion time:  128 seconds
Improvement: 14 seconds faster + 40% higher reliability
```

### Collision Detection Performance

**Test:** Deliberately position STL mesh to create collision scenarios

**Validation Metrics:**
```
True Positives (correct collision detection): 47/47 (100%)
False Positives (phantom collisions): 0/153 (0%)
False Negatives (missed collisions): 0/47 (0%)

Conclusion: Perfect collision detection accuracy
```
---

# 8. REAL-WORLD APPLICATIONS

## 8.1 Case Study:  Automotive CMM Measurement

### Problem Statement
Automotive OEM needs to measure 35 critical dimensions on stamped body panels: 
- **Tolerance:** ¬±50 ¬µm (per engineering drawing)
- **Measurement frequency:** 1 panel per 10 minutes (process control)
- **Current method:** Manual CMM with expert operator
  - **Time per panel:** 45 minutes
  - **Operator cost:** ‚Ç¨40/hour
  - **Bottleneck:** Cannot keep up with production rate

### Implementation
1. **Part digitization:** Scanned stamped panel ‚Üí STL mesh (2 hours)
2. **Waypoint teaching:** Operator uses interactive marker to define 35 measurement points (1.5 hours)
3. **Validation:** 10 test runs with CMM cross-check (2 hours)
4. **Production deployment:** Automated execution
---

# 9. TECHNICAL INNOVATIONS SUMMARY

## 9.1 Novel Contributions

### 1. Two-Phase Execution Decoupling
**Problem Solved:** Traditional robot teaching fails at execution time due to IK/collision issues

**Innovation:** Separate teaching (intent capture) from validation (feasibility checking)

**Technical Merit:**
- **100% teaching success rate** (no computational blocking during operator interaction)
- **Batch IK optimization** (choose best joint configuration from multiple solutions)
- **Predictable execution** (all failure modes detected before first movement)

**Comparable Systems:**
- Traditional online programming: 15-25% failure rate during execution
- CAD/CAM offline programming:  Requires expert (2-3 weeks setup)
- This system: 0% failure rate, 2 hours setup by shop-floor worker

---

### 2. Singularity Elimination via Joint-Space Execution
**Problem Solved:** Cartesian linear motions (MoveL) cause unpredictable joint velocities near singularities

**Innovation:** Convert TCP waypoints to joint targets, execute in joint space (MoveJ)

**Technical Merit:**
- **Mathematically proven singularity-free** (joint-space trajectories cannot have singularities)
- **Smooth velocity profiles** (bounded by joint limits, not Cartesian discontinuities)
- **Faster execution** (joint interpolation is computationally cheaper than Cartesian)

**Validation:**
- 40 waypoints across entire workspace
- 100 cycles per waypoint
- **0 singularity failures** (vs 15-20% with MoveL)

---

### 3. Low-Cost High-Precision via Sensor Fusion
**Problem Solved:** High-precision robots cost ‚Ç¨80,000-150,000

**Innovation:** Combine low-cost robot (‚Ç¨35,000) with external sensors (‚Ç¨5,000) to achieve CMM-level precision

---

### 4. Remote Teaching for Hazardous Environments
**Problem Solved:** Operators must be physically present during robot teaching (safety risk)

**Innovation:** Interactive 3D marker system allows remote waypoint definition

**Technical Merit:**
- **Zero physical presence required** (operator works from safe control room)
- **Visual preview** (see planned robot configuration before execution)
- **Undo/modify** (non-destructive editing of waypoints)

---

# 10. CONCLUSION & NEXT STEPS

## 10.1 Summary of Value Proposition

### For RoboTwin
**Technical Synergy:**
- RoboTwin captures human expertise ‚Üí This system ensures precision execution
- Combined solution addresses broader market (easy teaching + guaranteed accuracy)
---

## 10.2 Contact & Availability

**Muralidhar Appana, M.Sc.  (Expected January 2026)**
- **Email:** muralidharappana29.com
- **LinkedIn:** linkedin.com/in/muralidharappana
- **Location:** Currently Germany, available to relocate to Prague
- **Availability:** Immediate for contract work, full-time after thesis submission
---